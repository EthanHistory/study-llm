{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f4f6f9; padding: 20px; border-radius: 10px; text-align: center; font-family: 'Arial', sans-serif;\">\n",
    "    <h1 style=\"color: #2c3e50; font-size: 36px; margin-bottom: 10px;\">ðŸ“˜ Tokenizer Essentials</h1>\n",
    "    <h3 style=\"color: #34495e; font-size: 24px; margin-bottom: 5px;\">A Comprehensive Guide to <span style=\"color: #3498db;\">Tokenizer</span></h3>\n",
    "    <h4 style=\"color: #7f8c8d; font-size: 18px; margin-top: 0;\">Author: <span style=\"color: #16a085;\">Inseong Han</span></h4>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Tokenization\n",
    "\n",
    "Tokenization is a fundamental process in natural language processing (NLP) that involves breaking down text into smaller units, known as **tokens**. These tokens can be words, phrases, or even individual characters, depending on the use case. The goal of tokenization is to prepare text data for analysis and to enable machines to understand and process human language.\n",
    "\n",
    "This process is crucial for various applications such as text classification, sentiment analysis, and machine translation. By segmenting the text into manageable pieces, tokenization simplifies complex linguistic structures and lays the groundwork for more advanced tasks in NLP.\n",
    "\n",
    "> *\"Tokenization is the first step in teaching machines to read and understand language.\"*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center;\">\n",
    "  <!-- Left Side: Description -->\n",
    "  <div style=\"flex: 1; padding-right: 20px; font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "    <h2>Tokenizer Example</h2>\n",
    "    <p>\n",
    "      This example demonstrates how text inputs, including a <strong>System Prompt</strong> and a <strong>User Prompt</strong>, are tokenized and prepared for processing by a language model.\n",
    "    </p>\n",
    "    <ul>\n",
    "      <li><strong>Tokenization:</strong> The text is split into smaller pieces called tokens. These tokens can represent words, parts of words, or special symbols, depending on the tokenizer's rules. Tokenization helps break down the text into manageable units for the model to process.</li>\n",
    "      <li><strong>Special Tokens:</strong> You can see special tokens like <code>&lt;|im_start|&gt;</code>, <code>&lt;|im_sep|&gt;</code>, and <code>&lt;|im_end|&gt;</code>. These are used to structure the conversation, marking where the system, user, and assistant inputs start and end. They guide the model in understanding the context and flow of the interaction.</li>\n",
    "      <li><strong>Token Count:</strong> The total number of tokens (<strong>36</strong> in this case) is displayed. This count helps us track how much input is being processed, which is important because models have a limit on the number of tokens they can handle at once.</li>\n",
    "      <li><strong>Token Indices:</strong> Each token is assigned a unique number that corresponds to its position in the tokenizer's vocabulary. These numbers are what the model actually processes during computation.</li>\n",
    "    </ul>\n",
    "    <p>\n",
    "      This example provides insight into the critical steps of tokenization and how raw text is converted into a structured format that a machine learning model can understand and work with.\n",
    "    </p>\n",
    "  </div>\n",
    "  \n",
    "  <!-- Right Side: Image -->\n",
    "  <div style=\"flex: 1;\">\n",
    "    <img src=\"images/tokenizer_example.png\" alt=\"Tokenizer Example\" style=\"max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-Level Tokenization\n",
    "In character-level tokenization, the text is split into individual characters. This approach captures the finest level of granularity but may result in longer sequences and higher computational costs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'i', 's', ' ', 'f', 'u', 'n', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"Tokenization is fun!\"\n",
    "character_tokens = list(text)\n",
    "print(character_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-Level Tokenization\n",
    "In word-level tokenization, the text is split into words using spaces or punctuation as delimiters. It provides more meaningful tokens but might struggle with out-of-vocabulary words (e.g., rare or misspelled words).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'fun', '!']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Tokenization is fun!\"\n",
    "word_tokens = re.findall(r'\\w+|\\S', text)\n",
    "print(word_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Aspect**                 | **Character-Level**                                           | **Word-Level**                                               |\n",
    "|----------------------------|-------------------------------------------------------------|-------------------------------------------------------------|\n",
    "| **Granularity**            | Splits text into individual characters.                     | Splits text into words or sub-words.                       |\n",
    "| **Token Count**            | Produces more tokens for the same text (higher sequence length). | Produces fewer tokens (shorter sequence length).            |\n",
    "| **Meaningfulness**         | Tokens are often not meaningful on their own.              | Tokens usually have a clear meaning (words).               |\n",
    "| **Handling Out-of-Vocab**  | No issues, as every character is known.                    | Struggles with rare or unseen words (e.g., typos).          |\n",
    "| **Computational Cost**     | Higher, due to longer sequences.                           | Lower, due to shorter sequences.                           |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
